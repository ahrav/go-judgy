package domain

import (
	"errors"
	"time"

	"github.com/google/uuid"
)

// Answer-specific errors.
var (
	// ErrInvalidAnswer is returned when answer validation fails due to missing
	// required fields or constraint violations.
	ErrInvalidAnswer = errors.New("invalid answer")

	// ErrEmptyAnswers is returned when scoring operations receive no valid answers
	// to evaluate.
	ErrEmptyAnswers = errors.New("no answers provided")
)

// FinishReason indicates why the LLM stopped generating content.
// This provides explicit tracking of completion reasons for better observability.
type FinishReason string

const (
	// FinishStop indicates the model finished naturally (hit a stop token or completed the response).
	FinishStop FinishReason = "stop"

	// FinishLength indicates the response was truncated due to maximum length limits.
	FinishLength FinishReason = "length"

	// FinishContentFilter indicates the response was blocked by content filtering.
	FinishContentFilter FinishReason = "content_filter"

	// FinishToolUse indicates the model requested to use a tool (function calling).
	FinishToolUse FinishReason = "tool_use"
)

// AnswerProvenance captures the source and generation context of an answer.
// Groups all fields related to where and when the answer was generated.
type AnswerProvenance struct {
	// Provider identifies the LLM service that generated this answer (e.g., "openai", "anthropic").
	Provider string `json:"provider" validate:"required"`

	// Model specifies the exact model version used for generation.
	Model string `json:"model" validate:"required"`

	// GeneratedAt records the answer generation timestamp.
	GeneratedAt time.Time `json:"generated_at" validate:"required"`

	// TraceID allows correlation with external tracing systems.
	TraceID string `json:"trace_id,omitempty"`

	// ProviderRequestIDs tracks all provider request IDs for audit trails.
	ProviderRequestIDs []string `json:"provider_request_ids,omitempty"`
}

// AnswerUsage captures resource consumption metrics for an answer.
// Groups all fields related to computational costs and performance.
type AnswerUsage struct {
	// LatencyMillis measures end-to-end generation time in milliseconds.
	LatencyMillis int64 `json:"latency_ms" validate:"min=0"`

	// PromptTokens counts input tokens sent to the model.
	PromptTokens int64 `json:"prompt_tokens" validate:"min=0"`

	// CompletionTokens counts output tokens generated by the model.
	CompletionTokens int64 `json:"completion_tokens" validate:"min=0"`

	// TotalTokens equals PromptTokens plus CompletionTokens.
	TotalTokens int64 `json:"total_tokens" validate:"min=0"`

	// CallsUsed tracks the number of API calls made for this answer.
	// Includes retries and any intermediate calls.
	CallsUsed int64 `json:"calls_used" validate:"min=0"`
}

// AnswerCost captures the financial cost of generating an answer.
// Separated to enable fine-grained cost tracking and budgeting.
type AnswerCost struct {
	// EstimatedCost represents the generation cost using the Cents type.
	EstimatedCost Cents `json:"estimated_cost" validate:"min=0"`
}

// AnswerState captures the completion and error state of an answer.
// Groups all fields related to the success/failure status of generation.
type AnswerState struct {
	// FinishReason indicates why the LLM stopped generating content.
	FinishReason FinishReason `json:"finish_reason,omitempty"`

	// Truncated indicates if the response exceeded length limits.
	// This field is derived from FinishReason for backward compatibility.
	Truncated bool `json:"truncated"`

	// RetryCount tracks generation retry attempts.
	RetryCount int `json:"retry_count" validate:"min=0"`

	// Error contains the failure reason if generation unsuccessful.
	Error string `json:"error,omitempty"`
}

// Answer represents a single LLM-generated response with associated metadata
// and performance metrics.
// Answer content is stored externally via ContentRef to keep process audit trail lightweight.
// All monetary values are in cents to avoid floating-point precision issues.
// Fields are organized into embedded structs for better cohesion and maintainability.
type Answer struct {
	// ID uniquely identifies this answer in UUID format.
	ID string `json:"id" validate:"required,uuid"`

	// ContentRef points to the answer content stored as an external artifact.
	ContentRef ArtifactRef `json:"content_ref" validate:"required"`

	// AnswerProvenance contains source and generation context information.
	AnswerProvenance

	// AnswerUsage contains resource consumption metrics.
	AnswerUsage

	// AnswerCost contains financial cost information.
	AnswerCost

	// AnswerState contains completion and error state information.
	AnswerState

	// Metadata stores provider-specific supplementary information.
	Metadata map[string]any `json:"metadata,omitempty"`
}

// Validate checks structural integrity and constraint compliance.
// Returns nil if all required fields are present and constraints are satisfied,
// otherwise returns a validation error detailing the first violation found.
func (a *Answer) Validate() error { return validate.Struct(a) }

// IsValid reports whether the answer is suitable for scoring operations.
// Returns true when content is available (non-empty ContentRef.Key),
// ContentRef.Kind is correct for answers, and generation completed without errors.
func (a *Answer) IsValid() bool {
	return a.ContentRef.Key != "" && a.ContentRef.Kind == ArtifactAnswer && a.Error == ""
}

// CalculateTotalTokens updates TotalTokens to equal PromptTokens plus CompletionTokens.
// This method modifies the answer in-place and should be called after setting
// both token count fields.
func (a *Answer) CalculateTotalTokens() { a.TotalTokens = a.PromptTokens + a.CompletionTokens }

// UpdateTruncatedFromFinishReason sets the Truncated field based on the FinishReason.
// This provides backward compatibility while making truncation reasons explicit.
func (a *Answer) UpdateTruncatedFromFinishReason() {
	a.Truncated = a.FinishReason == FinishLength || a.FinishReason == FinishContentFilter
}

// NewAnswer creates a new Answer with a generated UUID and current timestamp.
// This constructor is suitable for most use cases where deterministic IDs are not required.
func NewAnswer(contentRef ArtifactRef, provider, model string) *Answer {
	return &Answer{
		ID:         uuid.New().String(),
		ContentRef: contentRef,
		AnswerProvenance: AnswerProvenance{
			Provider:    provider,
			Model:       model,
			GeneratedAt: time.Now(),
		},
	}
}

// MakeAnswer creates a new Answer with caller-supplied ID and timestamp.
// This constructor provides deterministic behavior needed for workflow systems
// like Temporal where reproducible execution is required.
func MakeAnswer(id string, generatedAt time.Time, contentRef ArtifactRef, provider, model string) *Answer {
	return &Answer{
		ID:         id,
		ContentRef: contentRef,
		AnswerProvenance: AnswerProvenance{
			Provider:    provider,
			Model:       model,
			GeneratedAt: generatedAt,
		},
	}
}

// GenerateAnswersInput specifies the parameters for LLM answer generation.
// Used as input to the GenerateAnswers evaluation operation.
type GenerateAnswersInput struct {
	// Question contains the prompt text for answer generation.
	// Must be 10-1000 characters to ensure meaningful responses while preventing abuse.
	Question string `json:"question" validate:"required,min=10,max=1000"`

	// NumAnswers specifies how many independent answers to generate.
	// Limited to 1-10 to balance evaluation quality with cost control.
	NumAnswers int `json:"num_answers" validate:"required,min=1,max=10"`

	// Config provides generation parameters including model selection and limits.
	Config EvalConfig `json:"config" validate:"required"`
}

// Validate verifies that all input parameters meet operation requirements.
// Returns validation error if Question length, NumAnswers range, or Config
// constraints are violated.
func (g *GenerateAnswersInput) Validate() error { return validate.Struct(g) }

// GenerateAnswersOutput contains the results from LLM answer generation.
// Returned by the GenerateAnswers evaluation operation with generation metrics.
type GenerateAnswersOutput struct {
	// Answers holds all successfully generated responses.
	// At least one answer must be present for successful generation.
	Answers []Answer `json:"answers" validate:"required,min=1"`

	// TokensUsed reports total tokens consumed across all generation attempts.
	TokensUsed int64 `json:"tokens_used" validate:"min=0"`

	// CallsMade counts the number of LLM API requests made during generation.
	CallsMade int64 `json:"calls_made" validate:"min=0"`

	// CostCents represents the total cost of generation in cents.
	// Used for budget reconciliation via BudgetClient.CommitSync.
	CostCents Cents `json:"cost_cents" validate:"min=0"`

	// ClientIdemKey is the canonical idempotency key from the LLM client.
	// Used as the single source of truth for event deduplication and caching.
	ClientIdemKey string `json:"client_idem_key,omitempty"`

	// Error contains the failure reason if no answers were successfully generated.
	Error string `json:"error,omitempty"`
}

// Validate ensures the output contains at least one answer and valid metrics.
// Returns validation error if required fields are missing or constraints violated.
func (g *GenerateAnswersOutput) Validate() error { return validate.Struct(g) }

// CalculateTotalTokens updates TokensUsed to equal the sum of TotalTokens
// from all answers in the collection.
// This method modifies the output in-place and should be called after
// all answers have been populated.
func (g *GenerateAnswersOutput) CalculateTotalTokens() {
	var total int64
	for _, answer := range g.Answers {
		total += answer.TotalTokens
	}
	g.TokensUsed = total
}
